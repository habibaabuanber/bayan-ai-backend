from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Optional, Any
import time
import json
from langdetect import detect

# Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
app = FastAPI(title="Book Recommendation API", version="1.0.0")

# Ø¥Ø¶Ø§ÙØ© CORS Ù„Ù„Ø³Ù…Ø§Ø­ Ø¨Ø§Ù„Ø·Ù„Ø¨Ø§Øª Ù…Ù† Ø§Ù„Ù…ØªØµÙØ­
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ÙÙŠ Ø§Ù„Ø¥Ù†ØªØ§Ø¬ ØºÙŠØ± Ù‡Ø°Ø§ Ø¥Ù„Ù‰ Ø£ØµÙˆÙ„ Ù…Ø­Ø¯Ø¯Ø©
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==================== MODELS ====================
class ChatRequest(BaseModel):
    message: Optional[str] = None
    session_id: Optional[str] = None
    reset: bool = False

class ChatResponse(BaseModel):
    session_id: str
    reply: str
    books: List[Dict[str, Any]]
    follow_up: bool


# ==================== ROUTES ====================
@app.get("/")
def read_root():
    """Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    return {
        "message": "Welcome to Book Recommendation API",
        "version": "1.0.0",
        "endpoints": {
            "chat": "POST /chat",
            "docs": "GET /docs",
            "health": "GET /health"
        }
    }

@app.get("/health")
def health_check():
    """ÙØ­Øµ ØµØ­Ø© Ø§Ù„Ø®Ø§Ø¯Ù…"""
    return {"status": "healthy", "sessions": len(SESSIONS)}
@app.post("/chat")
def chat(req: ChatRequest):
    """Ø§Ù„Ù€ endpoint Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù„Ø¯Ø±Ø¯Ø´Ø©"""
    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¬Ù„Ø³Ø§Øª Ø§Ù„Ù…Ù†ØªÙ‡ÙŠØ©
    sweep_expired_sessions()
    
    # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø·Ù„Ø¨ Ø§Ù„ÙˆØ§Ø±Ø¯
    log_incoming_request(req)
    
    # Ø¥Ø°Ø§ Ø·Ù„Ø¨ resetØŒ Ø£Ù†Ø´Ø¦ Ø¬Ù„Ø³Ø© Ø¬Ø¯ÙŠØ¯Ø©
    if req.reset:
        return handle_new_session()
    
    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¬Ù„Ø³Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©
    session_data = get_current_session(req.session_id)
    sid = session_data["sid"]
    session = session_data["session"]
    
    # Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù‡Ù†Ø§Ùƒ Ø±Ø³Ø§Ù„Ø©
    if not req.message:
        return handle_empty_message(sid)
    
    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø°ÙƒÙŠ
    return process_user_message(req.message, sid, session)
# ==================== GLOBALS ====================
SESSIONS = {}
SESSION_TIMEOUT = 1800  # 30 Ø¯Ù‚ÙŠÙ‚Ø©
TOP_K = 5

# ==================== SESSION UTILS ====================
def create_session() -> str:
    """Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ù„Ø³Ø© Ø¬Ø¯ÙŠØ¯Ø©"""
    from uuid import uuid4
    sid = str(uuid4())[:8]
    SESSIONS[sid] = {
        "created_at": time.time(),
        "last_activity": time.time(),
        "user_prefs": {},
        "conversation_history": [],
        "recommended": False,
    }
    return sid

def get_session(session_id: str) -> str:
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¬Ù„Ø³Ø© Ø£Ùˆ Ø¥Ù†Ø´Ø§Ø¡ ÙˆØ§Ø­Ø¯Ø© Ø¬Ø¯ÙŠØ¯Ø©"""
    if session_id not in SESSIONS:
        return create_session()
    return session_id

def touch_session(sid: str):
    """ØªØ­Ø¯ÙŠØ« ÙˆÙ‚Øª Ø§Ù„Ø¬Ù„Ø³Ø©"""
    if sid in SESSIONS:
        SESSIONS[sid]["last_activity"] = time.time()

def sweep_expired_sessions():
    """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¬Ù„Ø³Ø§Øª Ø§Ù„Ù…Ù†ØªÙ‡ÙŠØ©"""
    current_time = time.time()
    expired = [
        sid for sid, data in SESSIONS.items()
        if current_time - data["last_activity"] > SESSION_TIMEOUT
    ]
    for sid in expired:
        del SESSIONS[sid]
    if expired:
        print(f"ğŸ§¹ Swept {len(expired)} expired sessions")

# ==================== LANGUAGE UTILS ====================
def normalize_language(lang: str) -> str:
    """ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù„ØºØ©"""
    lang = lang.lower().strip()
    if lang in ["ar", "arabic", "Ø¹Ø±Ø¨ÙŠ", "Ø¹Ø±Ø¨ÙŠØ©"]:
        return "ar"
    elif lang in ["en", "english", "eng", "Ø§Ù†Ø¬Ù„ÙŠØ²ÙŠ", "Ø§Ù†Ø¬Ù„ÙŠØ²Ù‰"]:
        return "en"
    return "en"  # Default

def detect_and_normalize_language(text: str) -> Dict:
    """ÙƒØ´Ù Ø§Ù„Ù„ØºØ© ÙˆØªØ·Ø¨ÙŠØ¹Ù‡Ø§"""
    try:
        lang = "ar" if detect(text) == "ar" else "en"
    except:
        lang = "en"
    
    normalized_lang = normalize_language(lang)
    print(f"ğŸŒ Detected language: {lang} â†’ Normalized: {normalized_lang}")
    
    return {"detected": lang, "normalized": normalized_lang}

# ==================== CONVERSATION UTILS ====================
def get_last_assistant_message(conversation_history: List[Dict]) -> Optional[str]:
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¢Ø®Ø± Ø±Ø³Ø§Ù„Ø© Ù„Ù„Ù…Ø³Ø§Ø¹Ø¯"""
    for msg in reversed(conversation_history):
        if msg["role"] == "assistant":
            return msg["content"]
    return None

def update_conversation_history(session: Dict, role: str, content: str):
    """ØªØ­Ø¯ÙŠØ« Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©"""
    session["conversation_history"].append({"role": role, "content": content})

def save_user_preference(session: Dict, user_text: str):
    """Ø­ÙØ¸ ØªÙØ¶ÙŠÙ„Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
    pref_key = f"pref_{len(session['user_prefs']) + 1}"
    session["user_prefs"][pref_key] = user_text

# ==================== LOGGING UTILS ====================
def log_incoming_request(req: ChatRequest):
    """ØªØ³Ø¬ÙŠÙ„ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø·Ù„Ø¨ Ø§Ù„ÙˆØ§Ø±Ø¯"""
    print("\nğŸŸ¢ --- Incoming Request ---")
    print(f"Reset: {req.reset}")
    print(f"Session ID: {req.session_id}")
    print(f"Message: {req.message}")
    print("----------------------------\n")

def log_response(response_type: str, response: Dict):
    """ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø±Ø¯ Ø§Ù„ØµØ§Ø¯Ø±"""
    print(f"\nğŸ”µ --- Outgoing Response ({response_type}) ---")
    print(json.dumps(response, indent=2, ensure_ascii=False))
    print("-" * 50 + "\n")

# ==================== SESSION HANDLERS ====================
def handle_new_session():
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¬Ù„Ø³Ø© Ø¬Ø¯ÙŠØ¯Ø©"""
    new_sid = create_session()
    starter = "New chat started. Hi! What kind of books are you in the mood for?"
    
    print(f"ğŸŸ¡ [New Session Created] session_id={new_sid}")
    print("â¬†ï¸ Sending Starter Response\n")
    
    response = {
        "session_id": new_sid,
        "reply": starter,
        "books": [],
        "follow_up": True,
    }
    
    log_response("New Session", response)
    return response

def get_current_session(session_id: str):
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¬Ù„Ø³Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©"""
    sid = get_session(session_id)
    session = SESSIONS[sid]
    touch_session(sid)
    
    print(f"ğŸŸ¡ [Session Active] session_id={sid}")
    print(f"ğŸ§¾ Current user_prefs: {list(session['user_prefs'].values())}")
    
    return {"sid": sid, "session": session}

def handle_empty_message(sid: str):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø­Ø§Ù„Ø© Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ø±Ø³Ø§Ù„Ø©"""
    print("âšª No message provided â€” returning Ready response.\n")
    
    response = {
        "session_id": sid,
        "reply": "Ready.",
        "books": [],
        "follow_up": True
    }
    
    log_response("Empty Message", response)
    return response

# ==================== FEEDBACK HANDLERS ====================
def gpt_detect_response_to_recommendations(user_text: str, last_assistant_msg: str) -> bool:
    """Ø§Ø³ØªØ®Ø¯Ø§Ù… GPT Ù„Ù„ØªØ­Ù‚Ù‚ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø±Ø¯Ù‹Ø§ Ø¹Ù„Ù‰ Ø§Ù„ØªÙˆØµÙŠØ§Øª"""
    prompt = f"""
    The user just replied to your previous message.
    Assistant's last message: "{last_assistant_msg[:200]}..."
    User's message: "{user_text}"
    Question: Is the user giving feedback on the book recommendations in the assistant's message?
    Respond ONLY with "yes" or "no".
    """
    
    resp = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=10
    )
    
    gpt_response = resp.choices[0].message.content.strip().lower()
    result = (gpt_response == "yes")
    print(f"RESPONSE OF RECOMMENDATION (GPT): {result}")
    
    return result

def check_feedback_on_recommendations(user_text: str, session: Dict, last_assistant_msg: Optional[str]) -> Dict:
    """Ø§Ù„ØªØ­Ù‚Ù‚ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙŠØ±Ø¯ Ø¹Ù„Ù‰ Ø§Ù„ØªÙˆØµÙŠØ§Øª"""
    is_response_to_recommendations = False
    is_negative_feedback = False
    
    if session.get("recommended") and last_assistant_msg:
        try:
            is_response_to_recommendations = gpt_detect_response_to_recommendations(
                user_text, last_assistant_msg
            )
            
            if is_response_to_recommendations:
                is_negative_feedback = detect_negative_feedback(
                    user_text, session["conversation_history"], last_assistant_msg
                )
                print(f"ğŸ­ Detected response to recommendations - Negative: {is_negative_feedback}")
                
        except Exception as e:
            print(f"âŒ GPT failed to detect response: {e}")
    
    return {
        "is_response_to_recommendations": is_response_to_recommendations,
        "is_negative_feedback": is_negative_feedback
    }
def detect_negative_feedback(user_text: str, conversation_history: List[Dict], last_recommendation: str) -> bool:
    """ÙƒØ´Ù Ø§Ù„Ø±Ø¯ÙˆØ¯ Ø§Ù„Ø³Ù„Ø¨ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… LLM"""
    
    prompt = f"""
    Analyze if the user is expressing dissatisfaction or negative feedback about the book recommendations.
    
    Last recommendation: "{last_recommendation[:300]}"
    User's response: "{user_text}"
    
    Consider:
    1. Is the user saying they don't like the recommendations?
    2. Is the user saying the books are not relevant?
    3. Is the user expressing disappointment?
    4. Is the user asking for different books?
    
    Respond with ONLY "yes" or "no".
    
    Examples of negative feedback:
    - "I don't like these"
    - "These are not what I wanted"
    - "Not relevant to my interests"
    - "Can you suggest something else?"
    
    Examples of NOT negative feedback:
    - "Thanks, these look interesting"
    - "Tell me more about the first book"
    - "Do you have more options?"
    """
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1,
            max_tokens=10
        )
        
        result = response.choices[0].message.content.strip().lower()
        is_negative = result == "yes"
        print(f"ğŸ­ LLM Negative Feedback Detection: {is_negative} ({result})")
        return is_negative
        
    except Exception as e:
        print(f"âŒ LLM feedback detection failed: {e}")
        # Fallback Ø¥Ù„Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨Ø³ÙŠØ·
        return detect_negative_feedback_fallback(user_text)

def detect_negative_feedback_fallback(user_text: str) -> bool:
    """Fallback Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø±Ø¯ÙˆØ¯ Ø§Ù„Ø³Ù„Ø¨ÙŠØ©"""
    user_text_lower = user_text.lower()
    
    negative_indicators = [
        # English
        "don't like", "do not like", "not good", "not what i wanted",
        "not relevant", "not interested", "not helpful", "bad",
        "terrible", "awful", "horrible", "disappointed", "disappointing",
        "useless", "waste", "wrong", "incorrect", "not right",
        "try again", "start over", "different", "another", "other",
        "something else", "no thanks", "no thank you",
        
        # Arabic
        "Ù…Ø´ Ø¹Ø§Ø¬Ø¨Ù†ÙŠ", "Ù„Ø§ ÙŠØ¹Ø¬Ø¨Ù†ÙŠ", "Ù„ÙŠØ³ Ø¬ÙŠØ¯", "Ù„ÙŠØ³ Ù…Ø§ Ø£Ø±Ø¯Øª",
        "ØºÙŠØ± Ù…Ù†Ø§Ø³Ø¨", "ØºÙŠØ± Ù…Ù‡ØªÙ…", "ØºÙŠØ± Ù…ÙÙŠØ¯", "Ø³ÙŠØ¡",
        "ÙØ¸ÙŠØ¹", "Ù…Ø®ÙŠØ¨", "Ø®ÙŠØ¨Ø© Ø£Ù…Ù„", "ØºÙŠØ± ØµØ­ÙŠØ­",
        "Ø­Ø§ÙˆÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰", "Ø§Ø¨Ø¯Ø£ Ù…Ù† Ø¬Ø¯ÙŠØ¯", "Ù…Ø®ØªÙ„Ù", "Ø¢Ø®Ø±",
        "Ø´ÙŠØ¡ Ø¢Ø®Ø±", "Ù„Ø§ Ø´ÙƒØ±Ø§"
    ]
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…Ø¤Ø´Ø±Ø§Øª Ø³Ù„Ø¨ÙŠØ©
    has_negative = any(indicator in user_text_lower for indicator in negative_indicators)
    
    # Ø£ÙŠØ¶Ø§Ù‹ ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù†ÙÙŠ
    negation_words = ["not", "no", "don't", "doesn't", "isn't", "wasn't", "weren't", "Ù„Ø§", "Ù„ÙŠØ³", "Ù…Ø§", "Ù…Ø´"]
    has_negation = any(word in user_text_lower.split() for word in negation_words)
    
    # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Øµ Ù‚ØµÙŠØ±Ø§Ù‹ ÙˆÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù†ÙÙŠ
    is_short_negative = len(user_text.split()) < 5 and has_negation
    
    result = has_negative or is_short_negative
    print(f"ğŸ­ Fallback Negative Detection: {result} (has_negative: {has_negative}, is_short_negative: {is_short_negative})")
    
    return result


def handle_negative_feedback(user_text: str, sid: str, session: Dict, normalized_lang: str):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø¯ Ø§Ù„Ø³Ù„Ø¨ÙŠ Ø¹Ù„Ù‰ Ø§Ù„ØªÙˆØµÙŠØ§Øª - Ù…Ø­Ø¯Ø«Ø©"""
    print("ğŸŸ  [Stage] User dissatisfied with recommendations, generating smart follow-up...")
    
    # ØªØ­Ù„ÙŠÙ„ Ø³Ø¨Ø¨ Ø¹Ø¯Ù… Ø§Ù„Ø±Ø¶Ø§ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… LLM
    analysis_prompt = f"""
    The user expressed dissatisfaction with book recommendations.
    
    User's negative feedback: "{user_text}"
    Conversation context: {session['conversation_history'][-4:]}
    
    Analyze why the user might be dissatisfied:
    1. Are the books not relevant to their interests?
    2. Are the books not in their preferred language?
    3. Are the books not matching their preferred genre/type?
    4. Something else?
    
    Based on your analysis, suggest ONE helpful follow-up question that will help us understand what they really want.
    Write the question in {normalized_lang}.
    """
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": analysis_prompt}],
            temperature=0.7,
            max_tokens=150
        )
        
        reply = response.choices[0].message.content.strip()
        
    except Exception as e:
        print(f"âŒ Smart negative feedback analysis failed: {e}")
        # Fallback
        if normalized_lang == "ar":
            reply = "Ø¹Ø°Ø±Ø§Ù‹ØŒ ÙŠØ¨Ø¯Ùˆ Ø£Ù† Ø§Ù„ØªÙˆØµÙŠØ§Øª Ù„Ù… ØªÙƒÙ† Ù…Ù†Ø§Ø³Ø¨Ø©. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø®Ø¨Ø§Ø±ÙŠ Ø£ÙƒØ«Ø± Ø¹Ù† Ù†ÙˆØ¹ Ø§Ù„ÙƒØªØ¨ Ø§Ù„ØªÙŠ ØªØ¨Ø­Ø« Ø¹Ù†Ù‡Ø§ ØªØ­Ø¯ÙŠØ¯Ø§Ù‹ØŸ"
        else:
            reply = "I'm sorry the recommendations weren't right. Could you tell me more specifically what kind of books you're looking for?"
    
    # Ø­ÙØ¸ Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯
    update_conversation_history(session, "assistant", reply)
    touch_session(sid)
    
    response = {
        "session_id": sid,
        "reply": reply,
        "books": [],
        "follow_up": True
    }
    
    log_response("Smart Negative Feedback Response", response)
    return response
def gpt_detect_response_to_recommendations(user_text: str, last_assistant_msg: str) -> bool:
    """Ø§Ø³ØªØ®Ø¯Ø§Ù… GPT Ù„Ù„ØªØ­Ù‚Ù‚ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø±Ø¯Ù‹Ø§ Ø¹Ù„Ù‰ Ø§Ù„ØªÙˆØµÙŠØ§Øª"""
    if not last_assistant_msg:
        return False
    
    prompt = f"""
    Determine if the user is responding to/referencing the book recommendations in the assistant's last message.
    
    Assistant's last message: "{last_assistant_msg[:200]}..."
    User's message: "{user_text}"
    
    Respond with ONLY "yes" or "no".
    
    Consider:
    - Is the user commenting on the recommended books?
    - Is the user asking about specific books mentioned?
    - Is the user expressing opinion about the recommendations?
    - Is the user requesting changes/modifications to recommendations?
    """
    
    try:
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=10,
            temperature=0.1
        )
        
        gpt_response = resp.choices[0].message.content.strip().lower()
        result = (gpt_response == "yes")
        print(f"ğŸ¤– GPT Response Detection: {result} ({gpt_response})")
        return result
        
    except Exception as e:
        print(f"âŒ GPT detection failed: {e}")
        # Fallback Ø¨Ø³ÙŠØ·
        return simple_detect_response_to_recommendations(user_text, last_assistant_msg)

def simple_detect_response_to_recommendations(user_text: str, last_assistant_msg: str) -> bool:
    """ØªØ­Ù„ÙŠÙ„ Ø¨Ø³ÙŠØ· Ù„Ù„Ø±Ø¯ Ø¹Ù„Ù‰ Ø§Ù„ØªÙˆØµÙŠØ§Øª"""
    if not last_assistant_msg:
        return False
    
    user_lower = user_text.lower()
    last_msg_lower = last_assistant_msg.lower()
    
    # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø±Ø¯ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª Ù…Ø±ØªØ¨Ø·Ø© Ø¨Ø§Ù„ØªÙˆØµÙŠØ§Øª
    recommendation_keywords = [
        "book", "books", "recommend", "recommendation", "suggest", "suggestion",
        "title", "author", "this book", "these books", "that book",
        "ÙƒØªØ§Ø¨", "ÙƒØªØ¨", "Ø§Ù‚ØªØ±Ø§Ø­", "ØªÙˆØµÙŠØ©", "Ø±ÙˆØ§ÙŠØ©", "Ø¹Ù†ÙˆØ§Ù†", "Ù…Ø¤Ù„Ù"
    ]
    
    # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø£Ø®ÙŠØ±Ø© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª ØªÙˆØµÙŠØ©
    has_recommendation_in_last_msg = any(keyword in last_msg_lower for keyword in recommendation_keywords)
    
    # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø±Ø¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¥Ø´Ø§Ø±Ø© Ù„Ù„ØªÙˆØµÙŠØ§Øª
    has_response_to_recommendation = any(keyword in user_lower for keyword in recommendation_keywords)
    
    # Ø£Ùˆ Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠØ¹Ù„Ù‚ Ø¹Ù„Ù‰ Ø´ÙŠØ¡ Ù…Ø­Ø¯Ø¯ ÙÙŠ Ø§Ù„ØªÙˆØµÙŠØ§Øª
    is_commenting = any(word in user_lower for word in ["like", "don't like", "good", "bad", "interesting", "Ø¹Ø§Ø¬Ø¨", "Ù…Ø´ Ø¹Ø§Ø¬Ø¨"])
    
    return has_recommendation_in_last_msg and (has_response_to_recommendation or is_commenting)
def gpt_detect_response_to_recommendations(user_text: str, last_assistant_msg: str) -> bool:
    """Ø§Ø³ØªØ®Ø¯Ø§Ù… GPT Ù„Ù„ØªØ­Ù‚Ù‚ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø±Ø¯Ù‹Ø§ Ø¹Ù„Ù‰ Ø§Ù„ØªÙˆØµÙŠØ§Øª"""
    if not last_assistant_msg:
        return False
    
    prompt = f"""
    Determine if the user is responding to/referencing the book recommendations in the assistant's last message.
    
    Assistant's last message: "{last_assistant_msg[:200]}..."
    User's message: "{user_text}"
    
    Respond with ONLY "yes" or "no".
    
    Consider:
    - Is the user commenting on the recommended books?
    - Is the user asking about specific books mentioned?
    - Is the user expressing opinion about the recommendations?
    - Is the user requesting changes/modifications to recommendations?
    """
    
    try:
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=10,
            temperature=0.1
        )
        
        gpt_response = resp.choices[0].message.content.strip().lower()
        result = (gpt_response == "yes")
        print(f"ğŸ¤– GPT Response Detection: {result} ({gpt_response})")
        return result
        
    except Exception as e:
        print(f"âŒ GPT detection failed: {e}")
        # Fallback Ø¨Ø³ÙŠØ·
        return simple_detect_response_to_recommendations(user_text, last_assistant_msg)

def simple_detect_response_to_recommendations(user_text: str, last_assistant_msg: str) -> bool:
    """ØªØ­Ù„ÙŠÙ„ Ø¨Ø³ÙŠØ· Ù„Ù„Ø±Ø¯ Ø¹Ù„Ù‰ Ø§Ù„ØªÙˆØµÙŠØ§Øª"""
    if not last_assistant_msg:
        return False
    
    user_lower = user_text.lower()
    last_msg_lower = last_assistant_msg.lower()
    
    # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø±Ø¯ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª Ù…Ø±ØªØ¨Ø·Ø© Ø¨Ø§Ù„ØªÙˆØµÙŠØ§Øª
    recommendation_keywords = [
        "book", "books", "recommend", "recommendation", "suggest", "suggestion",
        "title", "author", "this book", "these books", "that book",
        "ÙƒØªØ§Ø¨", "ÙƒØªØ¨", "Ø§Ù‚ØªØ±Ø§Ø­", "ØªÙˆØµÙŠØ©", "Ø±ÙˆØ§ÙŠØ©", "Ø¹Ù†ÙˆØ§Ù†", "Ù…Ø¤Ù„Ù"
    ]
    
    # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø£Ø®ÙŠØ±Ø© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª ØªÙˆØµÙŠØ©
    has_recommendation_in_last_msg = any(keyword in last_msg_lower for keyword in recommendation_keywords)
    
    # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø±Ø¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¥Ø´Ø§Ø±Ø© Ù„Ù„ØªÙˆØµÙŠØ§Øª
    has_response_to_recommendation = any(keyword in user_lower for keyword in recommendation_keywords)
    
    # Ø£Ùˆ Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠØ¹Ù„Ù‚ Ø¹Ù„Ù‰ Ø´ÙŠØ¡ Ù…Ø­Ø¯Ø¯ ÙÙŠ Ø§Ù„ØªÙˆØµÙŠØ§Øª
    is_commenting = any(word in user_lower for word in ["like", "don't like", "good", "bad", "interesting", "Ø¹Ø§Ø¬Ø¨", "Ù…Ø´ Ø¹Ø§Ø¬Ø¨"])
    
    return has_recommendation_in_last_msg and (has_response_to_recommendation or is_commenting)
def check_feedback_on_recommendations(user_text: str, session: Dict, last_assistant_msg: Optional[str]) -> Dict:
    """Ø§Ù„ØªØ­Ù‚Ù‚ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙŠØ±Ø¯ Ø¹Ù„Ù‰ Ø§Ù„ØªÙˆØµÙŠØ§Øª - Ù…Ø­Ø¯Ø«Ø©"""
    is_response_to_recommendations = False
    is_negative_feedback = False
    
    if session.get("recommended") and last_assistant_msg:
        try:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… GPT Ù„Ù„ÙƒØ´Ù
            is_response_to_recommendations = gpt_detect_response_to_recommendations(
                user_text, last_assistant_msg
            )
            
            if is_response_to_recommendations:
                # Ø§Ø³ØªØ®Ø¯Ø§Ù… GPT Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ø³Ù„Ø¨ÙŠØ©
                is_negative_feedback = detect_negative_feedback(
                    user_text, session["conversation_history"], last_assistant_msg
                )
                print(f"ğŸ­ Detected response to recommendations - Negative: {is_negative_feedback}")
                
        except Exception as e:
            print(f"âŒ GPT detection failed: {e}")
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… fallback
            is_response_to_recommendations = simple_detect_response_to_recommendations(user_text, last_assistant_msg)
            if is_response_to_recommendations:
                is_negative_feedback = detect_negative_feedback_fallback(user_text)
    
    return {
        "is_response_to_recommendations": is_response_to_recommendations,
        "is_negative_feedback": is_negative_feedback
    }
    
# ==================== LANGUAGE HANDLERS ====================
def check_language_response(user_text: str, session: Dict, last_assistant_msg: Optional[str], normalized_lang: str):
    """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù„ØºØ©"""
    is_language_response = (
        last_assistant_msg and
        any(phrase in last_assistant_msg for phrase in [
            "Ø£ÙŠ Ù„ØºØ© ØªÙØ¶Ù„ Ø£Ù† ØªÙ‚Ø±Ø£ Ø¨Ù‡Ø§ Ø§Ù„ÙƒØªØ¨ØŸ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø£Ù… Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©ØŸ",
            "Which language do you prefer to read books in? Arabic or English?"
        ]) and
        any(word in user_text.lower() for word in [
            "english", "eng", "en", "Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©", "Ø§Ù†Ø¬Ù„ÙŠØ²ÙŠ", "Ø§Ù†Ø¬Ù„Ø´",
            "Ø¹Ø±Ø¨ÙŠ", "Ø¹Ø±Ø¨ÙŠØ©", "arabic", "ar"
        ])
    )
    
    if is_language_response and "preferred_reading_lang" not in session:
        return process_language_preference(user_text, session, normalized_lang)
    
    return None

def process_language_preference(user_text: str, session: Dict, normalized_lang: str):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© ØªÙØ¶ÙŠÙ„ Ù„ØºØ© Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©"""
    print("ğŸŸ¡ [Stage] Processing language preference...")
    
    # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…Ø®ØªØ§Ø±Ø©
    if any(word in user_text.lower() for word in [
        "english", "eng", "en", "Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©", "Ø§Ù†Ø¬Ù„ÙŠØ²ÙŠ", "Ø§Ù†Ø¬Ù„Ø´"
    ]):
        session["preferred_reading_lang"] = "en"
        confirmation = "Great! I'll recommend books in English ğŸ“š" if normalized_lang != "ar" else "Ø­Ø³Ù†Ø§Ù‹ØŒ Ø³Ø£ÙˆØµÙŠ Ù„Ùƒ Ø¨ÙƒØªØ¨ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© ğŸ“š"
    else:
        session["preferred_reading_lang"] = "ar"
        confirmation = "Great! I'll recommend books in Arabic ğŸ“š" if normalized_lang != "ar" else "Ø­Ø³Ù†Ø§Ù‹ØŒ Ø³Ø£ÙˆØµÙŠ Ù„Ùƒ Ø¨ÙƒØªØ¨ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ğŸ“š"
    
    # ØªÙˆÙ„ÙŠØ¯ Ø³Ø¤Ø§Ù„ Ù…ØªØ§Ø¨Ø¹Ø©
    follow_up = generate_contextual_followup(
        session["conversation_history"], normalized_lang
    )
    full_reply = f"{confirmation}\n\n{follow_up}"
    
    # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
    update_conversation_history(session, "assistant", full_reply)
    touch_session(session.get("sid", ""))
    
    # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø±Ø¯
    response = {
        "session_id": session.get("sid", ""),
        "reply": full_reply,
        "books": [],
        "follow_up": True
    }
    
    log_response("Language Confirmation", response)
    return response

def ask_preferred_language(sid: str, session: Dict, normalized_lang: str):
    """Ø³Ø¤Ø§Ù„ Ø¹Ù† Ù„ØºØ© Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…ÙØ¶Ù„Ø©"""
    print("ğŸŸ¡ [Stage] Asking for preferred reading language...")
    
    question = (
        "Ø£ÙŠ Ù„ØºØ© ØªÙØ¶Ù„ Ø£Ù† ØªÙ‚Ø±Ø£ Ø¨Ù‡Ø§ Ø§Ù„ÙƒØªØ¨ØŸ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø£Ù… Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©ØŸ"
        if normalized_lang == "ar"
        else "Which language do you prefer to read books in? Arabic or English?"
    )
    
    update_conversation_history(session, "assistant", question)
    touch_session(sid)
    
    response = {
        "session_id": sid,
        "reply": question,
        "books": [],
        "follow_up": True
    }
    
    log_response("Language Question", response)
    return response

# ==================== RECOMMENDATION HANDLERS ====================
def filter_books_by_language(books: List[Dict], target_lang: str) -> List[Dict]:
    """ØªØµÙÙŠØ© Ø§Ù„ÙƒØªØ¨ Ø­Ø³Ø¨ Ø§Ù„Ù„ØºØ©"""
    matched_books = []
    
    for book in books:
        book_lang_normalized = normalize_language(book.get('language', ''))
        if book_lang_normalized == target_lang:
            matched_books.append(book)
    
    # Ø¥Ø°Ø§ Ù„Ù… ØªÙˆØ¬Ø¯ ÙƒØªØ¨ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
    if not matched_books and books:
        matched_books = books[:2]
        print("âš ï¸ No books in preferred language, showing alternatives")
    
    return matched_books

def generate_recommendation_explanation(query: str, books: List[Dict], language: str) -> str:
    """ØªÙˆÙ„ÙŠØ¯ Ø´Ø±Ø­ Ù„Ù„ØªÙˆØµÙŠØ§Øª"""
    books_titles = [book['title'] for book in books]
    
    prompt = f"""
    You are a helpful librarian. The user described preferences: {query}
    Below are candidate books: {books_titles}
    
    For each book, write one short line in {language} explaining why it matches the user's preferences.
    Keep the response focused only on the books and their reasons.
    Start the recommendation with a short introductory sentence without hello or welcoming.
    Don't suggest non-existing books.
    Respond in {language}.
    """
    
    print("ğŸ¤– Sending prompt to LLM for recommendation explanation...")
    resp = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}]
    )
    
    reply = resp.choices[0].message.content
    print("âœ… [LLM Reply Received]")
    
    return reply

def generate_book_recommendations(sid: str, session: Dict, normalized_lang: str):
    """ØªÙˆÙ„ÙŠØ¯ ØªÙˆØµÙŠØ§Øª Ø§Ù„ÙƒØªØ¨"""
    print("ğŸŸ£ [Stage] Generating initial recommendations...")
    
    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù„ØºØ© Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…ÙØ¶Ù„Ø©
    reading_lang = session["preferred_reading_lang"]
    normalized_reading_lang = normalize_language(reading_lang)
    print(f"ğŸ“– Using preferred reading language: {reading_lang} â†’ Normalized: {normalized_reading_lang}")
    
    # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
    full_query = " ; ".join(session["user_prefs"].values())
    print(f"ğŸ“‹ Full user query: {full_query}")
    
    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙƒØªØ¨
    best_books = find_top_k(full_query, k=TOP_K)
    print(f"ğŸ“š Found {len(best_books)} similar books")
    
    # Ø¶Ù…Ø§Ù† ÙˆØ¬ÙˆØ¯ ØµÙˆØ± Ø§Ù„ØºÙ„Ø§Ù
    for book in best_books:
        ensure_cover(book)
    
    # ØªØµÙÙŠØ© Ø§Ù„ÙƒØªØ¨ Ø­Ø³Ø¨ Ø§Ù„Ù„ØºØ©
    matched_books = filter_books_by_language(
        best_books, normalized_reading_lang
    )
    
    # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø´Ø±Ø­
    explanation = generate_recommendation_explanation(
        full_query, matched_books, normalized_lang
    )
    
    # Ø¥Ø¶Ø§ÙØ© Ø³Ø¤Ø§Ù„ Ù…ØªØ§Ø¨Ø¹Ø©
    follow_up_question = generate_contextual_followup(
        session["conversation_history"], normalized_lang, is_after_recommendation=True
    )
    full_reply = f"{explanation}\n\n{follow_up_question}"
    
    # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
    update_conversation_history(session, "assistant", full_reply)
    session["recommended"] = True
    touch_session(sid)
    
    # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø±Ø¯
    response = {
        "session_id": sid,
        "reply": full_reply,
        "books": matched_books,
        "follow_up": True,
    }
    
    log_response("Initial Recommendation", response)
    return response

def generate_follow_up_question(sid: str, session: Dict, normalized_lang: str, is_after_recommendation: bool = False):
    """ØªÙˆÙ„ÙŠØ¯ Ø³Ø¤Ø§Ù„ Ù…ØªØ§Ø¨Ø¹Ø©"""
    print("ğŸŸ¢ [Stage] Generating follow-up question...")
    
    follow_up_question = generate_contextual_followup(
        session["conversation_history"], normalized_lang, is_after_recommendation
    )
    
    update_conversation_history(session, "assistant", follow_up_question)
    touch_session(sid)
    
    response = {
        "session_id": sid,
        "reply": follow_up_question,
        "books": [],
        "follow_up": True
    }
    
    log_response("Follow-up", response)
    return response

# ==================== MAIN MESSAGE PROCESSING ====================
def should_save_as_preference(user_text: str, session: Dict, last_assistant_msg: Optional[str]) -> bool:
    """ØªØ­Ø¯ÙŠØ¯ Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠØ¬Ø¨ Ø­ÙØ¸ Ø§Ù„Ù†Øµ ÙƒØªÙØ¶ÙŠÙ„"""
    user_text_lower = user_text.lower().strip()
    
    # Ù„Ø§ ØªØ­ÙØ¸ Ø¥Ø¬Ø§Ø¨Ø§Øª Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù„ØºØ©
    language_question_indicators = [
        "Ø£ÙŠ Ù„ØºØ© ØªÙØ¶Ù„",
        "which language",
        "arabic or english",
        "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø£Ù… Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©"
    ]
    
    if last_assistant_msg:
        last_msg_lower = last_assistant_msg.lower()
        if any(indicator in last_msg_lower for indicator in language_question_indicators):
            # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø±Ø¯ Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù„ØºØ©ØŒ Ù„Ø§ ØªØ­ÙØ¸Ù‡ ÙƒØªÙØ¶ÙŠÙ„
            if any(word in user_text_lower for word in ["arabic", "english", "Ø¹Ø±Ø¨ÙŠ", "Ø§Ù†Ø¬Ù„ÙŠØ²ÙŠ", "ar", "en"]):
                return False
    
    # Ù„Ø§ ØªØ­ÙØ¸ Ø±Ø¯ÙˆØ¯ Ø³Ù„Ø¨ÙŠØ© Ù…Ø«Ù„ "none", "Ù„Ø§", "nothing"
    negative_responses = ["none", "nothing", "Ù„Ø§", "Ù„ÙŠØ³ Ù„Ø¯ÙŠ", "Ù„Ø§ Ø£Ø¹Ø±Ù", "Ù„Ø§ ÙŠÙˆØ¬Ø¯"]
    if user_text_lower in negative_responses:
        return False
    
    # Ù„Ø§ ØªØ­ÙØ¸ Ø¥Ø¬Ø§Ø¨Ø§Øª Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø© Ø§Ù„Ø¹Ø§Ù…Ø©
    follow_up_keywords = ["what's the last book", "Ø¢Ø®Ø± ÙƒØªØ§Ø¨", "Ù…Ø§ Ù‡Ùˆ Ø¢Ø®Ø± ÙƒØªØ§Ø¨"]
    if last_assistant_msg and any(keyword in last_assistant_msg.lower() for keyword in follow_up_keywords):
        if len(user_text.split()) < 3:  # Ø¥Ø¬Ø§Ø¨Ø§Øª Ù‚ØµÙŠØ±Ø©
            return False
    
    # Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©: ØªØ­ÙØ¸ ÙÙ‚Ø· Ù†ØµÙˆØµ Ø°Ø§Øª Ù…Ø¹Ù†Ù‰
    if len(user_text.split()) < 2:  # ÙƒÙ„Ù…Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø·
        return False
    
    return True

def save_user_preference(session: Dict, user_text: str, last_assistant_msg: Optional[str] = None):
    """Ø­ÙØ¸ ØªÙØ¶ÙŠÙ„Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… - Ù…Ø­Ø³Ù†Ø©"""
    if not should_save_as_preference(user_text, session, last_assistant_msg):
        print(f"â© Skipping save: '{user_text}' (not a meaningful preference)")
        return
    
    pref_key = f"pref_{len(session['user_prefs']) + 1}"
    session["user_prefs"][pref_key] = user_text
    print(f"ğŸ’¾ Saved preference {pref_key}: '{user_text[:50]}...'")
def analyze_preferences_with_llm(conversation_history: List[Dict]) -> Dict:
    """ØªØ­Ù„ÙŠÙ„ Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… LLM Ù„ÙÙ‡Ù… Ø§Ù„ØªÙØ¶ÙŠÙ„Ø§Øª"""
    # Ø£Ø®Ø° Ø¢Ø®Ø± 6 Ø±Ø³Ø§Ø¦Ù„ Ù„Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
    recent_history = conversation_history[-6:]
    
    prompt = f"""
    Analyze this book recommendation conversation and extract:
    1. What the user wants (genres, topics, specific interests)
    2. What the user does NOT want (dislikes, things to avoid)
    3. Reading language preference (Arabic/English)
    4. Key search terms for finding books
    
    Conversation:
    {format_conversation(recent_history)}
    
    Return a JSON with this structure:
    {{
        "wants": ["list", "of", "topics", "genres"],
        "does_not_want": ["things", "to", "avoid"],
        "language": "ar" or "en",
        "search_terms": ["keywords", "for", "search"],
        "summary": "brief summary of preferences"
    }}
    
    Be specific. If the user mentions "pharaohs", include "ancient egypt", "egyptian history", etc.
    """
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",  # Ø£Ùˆ gpt-4o Ø¥Ø°Ø§ ØªØ±ÙŠØ¯
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"},
            temperature=0.3
        )
        
        analysis = json.loads(response.choices[0].message.content)
        print(f"ğŸ¤– LLM Analysis: {analysis}")
        return analysis
        
    except Exception as e:
        print(f"âŒ LLM analysis failed: {e}")
        # Fallback Ø¨Ø³ÙŠØ·
        return {
            "wants": [],
            "does_not_want": [],
            "language": "ar",
            "search_terms": extract_keywords_fallback(conversation_history),
            "summary": "User preferences"
        }
def smart_recommendation_system(sid: str, session: Dict, normalized_lang: str):
    """Ù†Ø¸Ø§Ù… ØªÙˆØµÙŠØ§Øª Ø°ÙƒÙŠ ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ LLM"""
    print("ğŸ§  [Stage] Using intelligent recommendation system...")
    
    # Step 1: ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙØ¶ÙŠÙ„Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… LLM
    preferences_analysis = analyze_preferences_with_llm(session["conversation_history"])
    
    # ØªØ­Ø¯ÙŠØ« Ù„ØºØ© Ø§Ù„Ø¬Ù„Ø³Ø© Ø¥Ø°Ø§ ØªÙ… Ø§ÙƒØªØ´Ø§ÙÙ‡Ø§
    detected_lang = preferences_analysis.get("language", normalized_lang)
    if detected_lang and "preferred_reading_lang" not in session:
        session["preferred_reading_lang"] = detected_lang
    
    # Step 2: Ø¨Ù†Ø§Ø¡ Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø°ÙƒÙŠ
    meaningful_prefs = [
        pref for pref in session["user_prefs"].values() 
        if len(pref.split()) > 1
    ]
    
    user_query = " ".join(meaningful_prefs) if meaningful_prefs else "books"
    
    # Step 3: Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø°ÙƒÙŠ
    reading_lang = session.get("preferred_reading_lang", detected_lang)
    matched_books = intelligent_book_search(
        user_query, 
        preferences_analysis, 
        reading_lang
    )
    
    # Step 4: Ø¥Ø°Ø§ Ù„Ù… ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬ØŒ Ø­Ø§ÙˆÙ„ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ù…Ø®ØªÙ„ÙØ©
    if not matched_books:
        print("ğŸ”„ No books found, trying alternative strategies...")
        
        # Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© 1: Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙƒÙ„Ù…Ø§Øª Ø£Ø¹Ù…Ù‚
        broader_search_terms = generate_broader_search_terms(preferences_analysis)
        for term in broader_search_terms[:3]:
            matched_books = intelligent_book_search(term, preferences_analysis, reading_lang)
            if matched_books:
                break
        
        # Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© 2: Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø£Ø®Ø±Ù‰
        if not matched_books and reading_lang == "ar":
            print("ğŸ”„ Trying English books as fallback...")
            matched_books = intelligent_book_search(
                user_query, 
                preferences_analysis, 
                "en"
            )
    
    # Step 5: ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø´Ø±Ø­ Ø§Ù„Ø°ÙƒÙŠ
    if matched_books:
        explanation = generate_intelligent_explanation(
            matched_books, 
            preferences_analysis, 
            normalized_lang
        )
        
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
        update_conversation_history(session, "assistant", explanation)
        session["recommended"] = True
        touch_session(sid)
        
        response = {
            "session_id": sid,
            "reply": explanation,
            "books": matched_books,
            "follow_up": True,
        }
        
        log_response("Intelligent Recommendation", response)
        return response
    
    else:
        # Ø¥Ø°Ø§ Ù„Ù… ØªÙˆØ¬Ø¯ ÙƒØªØ¨ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø·Ù„Ø§Ù‚
        return generate_no_books_smart_response(sid, session, preferences_analysis, normalized_lang)

def generate_intelligent_explanation(books: List[Dict], preferences: Dict, language: str) -> str:
    """ØªÙˆÙ„ÙŠØ¯ Ø´Ø±Ø­ Ù…Ø®ØªØµØ± ÙˆØ¬Ø°Ø§Ø¨ Ù„Ù„ØªÙˆØµÙŠØ§Øª"""
    
    books_info = []
    for book in books[:4]:  # 4 ÙƒØªØ¨ ÙƒØ­Ø¯ Ø£Ù‚ØµÙ‰
        title = book.get("title", "Unknown Book") or "Unknown Book"
        authors = book.get("authors", "Unknown Author") or "Unknown Author"
        
        books_info.append({
            "title": title,
            "authors": authors
        })
    
    prompt = f"""
    You are a helpful book recommender. Write a SHORT, engaging response.
    
    User is looking for: {preferences.get('summary', 'interesting books')}
    
    Books you're recommending (show only titles and authors):
    {json.dumps(books_info, ensure_ascii=False)}
    
    Write a concise message that:
    1. Starts with a friendly greeting (1 line max)
    2. Mentions you found some books matching their interests
    3. Lists the books clearly (title by author)
    4. Ends with ONE simple question to continue
    
    Keep it VERY SHORT - maximum 8-10 lines total.
    Be enthusiastic but concise.
    Write in {language}.
    
    Example format:
    "Great! Based on your interest in Victorian romance, here are some recommendations:
    
    1. Jane Eyre by Charlotte BrontÃ«
    2. Pride and Prejudice by Jane Austen
    
    Which one sounds most interesting to you?"
    """
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",  # Ø§Ø³ØªØ®Ø¯Ø§Ù… mini Ù„Ù„Ø£Ù‚ØµØ±
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=200  # ğŸ‘ˆ Ù‡Ø°Ø§ Ø§Ù„Ù…Ù‡Ù…! ÙŠØ­Ø¯Ø¯ Ø§Ù„Ø·ÙˆÙ„
        )
        
        reply = response.choices[0].message.content.strip()
        print(f"ğŸ“ Reply length: {len(reply)} characters")
        return reply
        
    except Exception as e:
        print(f"âŒ Intelligent explanation failed: {e}")
        # Fallback Ù…Ø®ØªØµØ±
        return generate_short_fallback_explanation(books, preferences, language)

def generate_short_fallback_explanation(books: List[Dict], preferences: Dict, language: str) -> str:
    """Ø´Ø±Ø­ Ù…Ø®ØªØµØ± fallback"""
    
    if language == "ar":
        lines = ["Ø¹Ø«Ø±Øª Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ø§Ù„ÙƒØªØ¨ Ø§Ù„ØªÙŠ Ù‚Ø¯ ØªØ¹Ø¬Ø¨Ùƒ:"]
        
        for i, book in enumerate(books[:3], 1):
            title = book.get("title", "ÙƒØªØ§Ø¨")
            authors = book.get("authors", "Ù…Ø¤Ù„Ù")
            lines.append(f"{i}. {title} - {authors}")
        
        lines.append("Ø£ÙŠ Ù…Ù†Ù‡Ø§ ÙŠÙ„ÙØª Ø§Ù†ØªØ¨Ø§Ù‡ÙƒØŸ")
        return "\n".join(lines)
    
    else:
        lines = ["Great! Here are some books you might like:"]
        
        for i, book in enumerate(books[:3], 1):
            title = book.get("title", "Book")
            authors = book.get("authors", "Author")
            lines.append(f"{i}. {title} by {authors}")
        
        lines.append("Which one catches your eye?")
        return "\n".join(lines)
    
def generate_broader_search_terms(preferences: Dict) -> List[str]:
    """ØªÙˆÙ„ÙŠØ¯ Ù…ØµØ·Ù„Ø­Ø§Øª Ø¨Ø­Ø« Ø£ÙˆØ³Ø¹"""
    
    prompt = f"""
    Based on these user preferences, suggest 5 broader or related search terms for finding books:
    
    User wants: {preferences.get('wants', [])}
    
    For example, if user wants "pharaohs history", broader terms could be:
    - ancient egypt civilization
    - egyptian archaeology
    - history of ancient egypt
    - egyptian pharaohs and pyramids
    - ancient world history
    
    Return ONLY a JSON array of strings, nothing else.
    """
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"},
            temperature=0.4
        )
        
        result = json.loads(response.choices[0].message.content)
        if isinstance(result, dict) and "terms" in result:
            return result["terms"]
        elif isinstance(result, list):
            return result
        else:
            return []
            
    except Exception as e:
        print(f"âŒ Broad terms generation failed: {e}")
        return []
    
def format_conversation(history: List[Dict]) -> str:
    """ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ù„Ù„Ù†Ù…ÙˆØ°Ø¬"""
    formatted = []
    for msg in history:
        role = "User" if msg["role"] == "user" else "Assistant"
        formatted.append(f"{role}: {msg['content']}")
    return "\n".join(formatted)

def process_user_message(message: str, sid: str, session: Dict):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø°ÙƒÙŠ - ÙƒØ§Ù…Ù„Ø©"""
    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ
    user_text = message.strip()
    
    # ØªØ­Ø¯ÙŠØ« Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
    update_conversation_history(session, "user", user_text)
    
    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¢Ø®Ø± Ø±Ø³Ø§Ù„Ø© Ù„Ù„Ù…Ø³Ø§Ø¹Ø¯
    last_assistant_msg = get_last_assistant_message(session["conversation_history"])
    
    # Ø­ÙØ¸ ØªÙØ¶ÙŠÙ„Ø§Øª Ø°ÙƒÙŠ
    save_user_preference_smart(session, user_text, last_assistant_msg)
    
    # ÙƒØ´Ù Ø§Ù„Ù„ØºØ©
    lang_info = detect_and_normalize_language(user_text)
    normalized_lang = lang_info["normalized"]
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø±Ø¯Ù‹Ø§ Ø¹Ù„Ù‰ Ø§Ù„ØªÙˆØµÙŠØ§Øª (Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø±Ø¯ÙˆØ¯ Ø§Ù„Ø³Ù„Ø¨ÙŠØ©)
    if session.get("recommended") and last_assistant_msg:
        feedback_info = check_feedback_on_recommendations(
            user_text, session, last_assistant_msg
        )
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø¯ Ø§Ù„Ø³Ù„Ø¨ÙŠ Ø¹Ù„Ù‰ Ø§Ù„ØªÙˆØµÙŠØ§Øª
        if feedback_info.get("is_negative_feedback"):
            return handle_negative_feedback(user_text, sid, session, normalized_lang)
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù„ØºØ©
    language_response_result = check_language_response(
        user_text, session, last_assistant_msg, normalized_lang
    )
    if language_response_result:
        return language_response_result
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„ÙˆÙ‚Øª Ù…Ù†Ø§Ø³Ø¨Ù‹Ø§ Ù„Ù„ØªÙˆØµÙŠØ§Øª
    need_recommend = check_if_ready_for_recommendations(session)
    
    if need_recommend:
        # Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø°ÙƒÙŠ Ù„Ù„ØªÙˆØµÙŠØ§Øª
        return smart_recommendation_system(sid, session, normalized_lang)
    else:
        # Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø°ÙƒÙŠ Ù„Ù„Ù…ØªØ§Ø¨Ø¹Ø©
        return generate_smart_follow_up(sid, session, normalized_lang)

def save_user_preference_smart(session: Dict, user_text: str, last_assistant_msg: Optional[str]):
    """Ø­ÙØ¸ ØªÙØ¶ÙŠÙ„Ø§Øª Ø°ÙƒÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… LLM"""
    # Ù„Ø§ ØªØ­ÙØ¸ Ù†ØµÙˆØµ Ù‚ØµÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹
    if len(user_text.strip().split()) < 2:
        print(f"â© Skipping short text: '{user_text}'")
        return
    
    # Ù„Ø§ ØªØ­ÙØ¸ Ø¥Ø¬Ø§Ø¨Ø§Øª Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù„ØºØ©
    if last_assistant_msg and any(phrase in last_assistant_msg.lower() for phrase in [
        "which language", "Ø£ÙŠ Ù„ØºØ©", "arabic or english", "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø£Ù… Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©"
    ]):
        if any(word in user_text.lower() for word in ["arabic", "english", "Ø¹Ø±Ø¨ÙŠ", "Ø§Ù†Ø¬Ù„ÙŠØ²ÙŠ"]):
            print(f"â© Skipping language response: '{user_text}'")
            # Ù„ÙƒÙ† Ø§Ø­ÙØ¸ Ø§Ù„Ù„ØºØ©
            if "arabic" in user_text.lower() or "Ø¹Ø±Ø¨ÙŠ" in user_text.lower():
                session["preferred_reading_lang"] = "ar"
            elif "english" in user_text.lower() or "Ø§Ù†Ø¬Ù„ÙŠØ²ÙŠ" in user_text.lower():
                session["preferred_reading_lang"] = "en"
            return
    
    # Ø§Ø³ØªØ®Ø¯Ø§Ù… LLM Ù„Ù„ØªØ­Ù‚Ù‚ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Øµ ØªÙØ¶ÙŠÙ„Ø§Ù‹ Ø­Ù‚ÙŠÙ‚ÙŠØ§Ù‹
    try:
        prompt = f"""
        Determine if this user message contains book preferences or interests that should be saved for book recommendations.
        
        Message: "{user_text}"
        Context: Last assistant message was: "{last_assistant_msg[:100] if last_assistant_msg else 'None'}"
        
        Respond with ONLY "yes" or "no".
        
        Save as preference if:
        - User mentions genres, topics, or types of books they like
        - User describes what they're looking for in a book
        - User shares reading preferences
        
        Do NOT save if:
        - It's a simple greeting or acknowledgment
        - It's a response to a specific question without new preferences
        - It's negative feedback about previous recommendations
        """
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1,
            max_tokens=10
        )
        
        should_save = response.choices[0].message.content.strip().lower() == "yes"
        
        if should_save:
            pref_key = f"pref_{len(session['user_prefs']) + 1}"
            session["user_prefs"][pref_key] = user_text
            print(f"ğŸ¤– LLM decided to save preference {pref_key}: '{user_text[:50]}...'")
        else:
            print(f"ğŸ¤– LLM decided NOT to save: '{user_text}'")
            
    except Exception as e:
        print(f"âŒ LLM preference check failed: {e}")
        # Fallback Ø¥Ù„Ù‰ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨Ø³ÙŠØ·Ø©
        if should_save_preference_fallback(user_text, last_assistant_msg):
            pref_key = f"pref_{len(session['user_prefs']) + 1}"
            session["user_prefs"][pref_key] = user_text
            print(f"ğŸ’¾ Fallback saved preference {pref_key}: '{user_text[:50]}...'")

def should_save_preference_fallback(user_text: str, last_assistant_msg: Optional[str]) -> bool:
    """Fallback Ù„Ù„ØªØ­Ù‚Ù‚ Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠØ¬Ø¨ Ø­ÙØ¸ Ø§Ù„Ù†Øµ ÙƒØªÙØ¶ÙŠÙ„"""
    user_text = user_text.strip()
    
    # Ù„Ø§ ØªØ­ÙØ¸ Ù†ØµÙˆØµ ÙØ§Ø±ØºØ© Ø£Ùˆ Ù‚ØµÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹
    if not user_text or len(user_text) < 3:
        return False
    
    # Ù„Ø§ ØªØ­ÙØ¸ Ø¥Ø¬Ø§Ø¨Ø§Øª Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ù…Ø­Ø¯Ø¯Ø©
    if last_assistant_msg:
        last_msg_lower = last_assistant_msg.lower()
        
        # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ù„ØºØ©
        if any(phrase in last_msg_lower for phrase in [
            "which language", "Ø£ÙŠ Ù„ØºØ©", "arabic or english", "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø£Ù… Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©"
        ]):
            return False
        
        # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø³Ø¤Ø§Ù„ Ù†Ø¹Ù…/Ù„Ø§ Ø¨Ø³ÙŠØ·
        if last_assistant_msg.endswith("?") and len(user_text.split()) < 3:
            simple_questions = ["yes", "no", "y", "n", "ok", "okay", "Ù†Ø¹Ù…", "Ù„Ø§", "Ø­Ø³Ù†Ø§", "Ø·ÙŠØ¨"]
            if user_text.lower() in simple_questions:
                return False
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù†ÙˆØ¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
    words = user_text.lower().split()
    
    # Ù„Ø§ ØªØ­ÙØ¸ Ù…Ø­ØªÙˆÙ‰ ØºÙŠØ± Ù…ØªØ¹Ù„Ù‚ Ø¨Ø§Ù„ÙƒØªØ¨
    unrelated_keywords = [
        "hello", "hi", "hey", "thanks", "thank you", "bye", "goodbye",
        "Ù…Ø±Ø­Ø¨Ø§", "Ø§Ù‡Ù„Ø§", "Ø´ÙƒØ±Ø§", "Ù…Ø¹ Ø§Ù„Ø³Ù„Ø§Ù…Ø©", "ÙˆØ¯Ø§Ø¹Ø§"
    ]
    
    if any(keyword in user_text.lower() for keyword in unrelated_keywords):
        return False
    
    # ØªØ­Ù‚Ù‚ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Øµ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ø§Ù„ÙƒØªØ¨
    book_related_keywords = [
        # English
        "book", "books", "novel", "story", "stories", "read", "reading",
        "genre", "author", "fiction", "non-fiction", "history", "historical",
        "romance", "mystery", "fantasy", "science", "biography", "poetry",
        "adventure", "crime", "thriller", "horror", "classic", "modern",
        
        # Arabic
        "ÙƒØªØ§Ø¨", "ÙƒØªØ¨", "Ø±ÙˆØ§ÙŠØ©", "Ù‚ØµØ©", "Ù‚ØµØµ", "Ù‚Ø±Ø§Ø¡Ø©", "Ù…Ø·Ø§Ù„Ø¹Ø©",
        "Ù†ÙˆØ¹", "Ù…Ø¤Ù„Ù", "Ø®ÙŠØ§Ù„", "ÙˆØ§Ù‚Ø¹ÙŠ", "ØªØ§Ø±ÙŠØ®", "ØªØ§Ø±ÙŠØ®ÙŠ",
        "Ø±ÙˆÙ…Ø§Ù†Ø³ÙŠ", "ØºÙ…ÙˆØ¶", "Ø®ÙŠØ§Ù„ Ø¹Ù„Ù…ÙŠ", "Ø³ÙŠØ±Ø©", "Ø´Ø¹Ø±", "Ù…ØºØ§Ù…Ø±Ø©",
        "Ø¬Ø±ÙŠÙ…Ø©", "Ø¥Ø«Ø§Ø±Ø©", "Ø±Ø¹Ø¨", "ÙƒÙ„Ø§Ø³ÙŠÙƒÙŠ", "Ø­Ø¯ÙŠØ«"
    ]
    
    # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Øµ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ø§Ù„ÙƒØªØ¨
    has_book_terms = any(term in user_text.lower() for term in book_related_keywords)
    
    # Ø£Ùˆ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Øµ Ø·ÙˆÙŠÙ„Ø§Ù‹ Ø¨Ù…Ø§ ÙÙŠÙ‡ Ø§Ù„ÙƒÙØ§ÙŠØ©
    is_long_enough = len(words) >= 3
    
    # Ø£Ùˆ Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙˆØµÙ Ø£Ùˆ Ø·Ù„Ø¨
    has_request = any(word in user_text.lower() for word in ["want", "need", "looking for", "Ø£Ø±ÙŠØ¯", "Ø£Ø­ØªØ§Ø¬", "Ø£Ø¨Ø­Ø« Ø¹Ù†"])
    
    return has_book_terms or is_long_enough or has_request
           
def check_if_ready_for_recommendations(session: Dict) -> bool:
    """Ø§Ù„ØªØ­Ù‚Ù‚ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø¬Ø§Ù‡Ø²Ø§Ù‹ Ù„Ù„ØªÙˆØµÙŠØ§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªØ­Ù„ÙŠÙ„ Ø°ÙƒÙŠ"""
    # Ø§Ù„Ø´Ø±Ø· Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ: 3 ØªÙØ¶ÙŠÙ„Ø§Øª Ø°Ø§Øª Ù…Ø¹Ù†Ù‰
    meaningful_prefs = [
        pref for pref in session["user_prefs"].values() 
        if len(pref.split()) > 1
    ]
    
    if len(meaningful_prefs) >= 3:
        return True
    
    # Ø£Ùˆ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù„Ø¯Ù‰ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø·Ù„Ø¨ Ù…Ø­Ø¯Ø¯
    last_user_msg = None
    for msg in reversed(session["conversation_history"]):
        if msg["role"] == "user":
            last_user_msg = msg["content"]
            break
    
    if last_user_msg:
        # ØªØ­Ù‚Ù‚ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø·Ù„Ø¨ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ©
        keywords = ["need", "want", "looking for", "Ø£Ø±ÙŠØ¯", "Ø£Ø­ØªØ§Ø¬", "Ø£Ø¨Ø­Ø« Ø¹Ù†"]
        book_terms = ["book", "books", "novel", "story", "ÙƒØªØ§Ø¨", "ÙƒØªØ¨", "Ø±ÙˆØ§ÙŠØ©"]
        
        has_request = any(kw in last_user_msg.lower() for kw in keywords)
        has_book_ref = any(term in last_user_msg.lower() for term in book_terms)
        
        if has_request and has_book_ref:
            return True
    
    return False

def generate_smart_follow_up(sid: str, session: Dict, language: str):
    """ØªÙˆÙ„ÙŠØ¯ Ø³Ø¤Ø§Ù„ Ù…ØªØ§Ø¨Ø¹Ø© Ø°ÙƒÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… LLM"""
    
    prompt = f"""
    You're helping someone find books. Based on this conversation, ask ONE natural follow-up question 
    to understand their book preferences better.
    
    Conversation history (recent):
    {format_conversation(session["conversation_history"][-4:])}
    
    Ask a question that:
    1. Is relevant to what they've already said
    2. Helps narrow down book recommendations
    3. Is conversational and friendly
    4. In {language} language
    
    Return ONLY the question, nothing else.
    """
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=100
        )
        
        question = response.choices[0].message.content.strip()
        
    except Exception as e:
        print(f"âŒ Smart follow-up failed: {e}")
        # Fallback
        if language == "ar":
            question = "Ù…Ø§ Ù†ÙˆØ¹ Ø§Ù„ÙƒØªØ¨ Ø§Ù„ØªÙŠ ØªØ³ØªÙ…ØªØ¹ Ø¨Ù‚Ø±Ø§Ø¡ØªÙ‡Ø§ Ø¹Ø§Ø¯Ø©ØŸ"
        else:
            question = "What type of books do you usually enjoy reading?"
    
    update_conversation_history(session, "assistant", question)
    touch_session(sid)
    
    response = {
        "session_id": sid,
        "reply": question,
        "books": [],
        "follow_up": True
    }
    
    log_response("Smart Follow-up", response)
    return response
def generate_contextual_followup(conversation_history: List[Dict], user_lang: str, is_after_recommendation: bool = False) -> str:
    """Ø¨ØªÙˆÙ„Ø¯ Ø£Ø³Ø¦Ù„Ø© Ù…ØªØ§Ø¨Ø¹Ø© Ø°ÙƒÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ context Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©"""
    # Ù†Ø¬Ù…Ø¹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
    history_text = "\n".join([f"{h['role']}: {h['content']}" for h in conversation_history[-6:]])  # Ø¢Ø®Ø± 3 ØªØ¨Ø§Ø¯Ù„Ø§Øª
    
    if is_after_recommendation:
        prompt = f"""
        Based on this conversation, generate ONE natural follow-up question to understand why the user might not be satisfied with the recommendations and what they'd prefer instead.
        Conversation: {history_text}
        Requirements:
        - Ask ONE question only
        - Be curious and helpful, not repetitive
        - Focus on understanding their specific taste better
        - Respond in {user_lang}
        """
    else:
        prompt = f"""
        Based on this conversation, generate ONE natural follow-up question that helps understand the user's book preferences better.
        Conversation: {history_text}
        Requirements:
        - Ask ONE question only
        - Be natural and conversational
        - Don't repeat previous questions
        - Respond in {user_lang}
        """
    
    try:
        resp = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=100
        )
        return resp.choices[0].message.content.strip()
    except:
        # Fallback questions
        import random
        fallback_questions_ar = [
            "Ù…Ø§ Ù‡Ùˆ Ø¢Ø®Ø± ÙƒØªØ§Ø¨ Ù‚Ø±Ø£ØªÙ‡ ÙˆØ£Ø¹Ø¬Ø¨ÙƒØŸ",
            "Ù‡Ù„ ØªÙØ¶Ù„ Ø§Ù„Ù‚ØµØµ Ø§Ù„Ø®ÙŠØ§Ù„ÙŠØ© Ø£Ù… Ø§Ù„ÙˆØ§Ù‚Ø¹ÙŠØ©ØŸ",
            "Ø£ÙŠ Ù†ÙˆØ¹ Ù…Ù† Ø§Ù„Ø´Ø®ØµÙŠØ§Øª ØªØ¬Ø°Ø¨Ùƒ Ø£ÙƒØ«Ø± ÙÙŠ Ø§Ù„Ø±ÙˆØ§ÙŠØ§ØªØŸ",
            "Ù…Ø§ Ø§Ù„Ù…Ø²Ø§Ø¬ Ø§Ù„Ø°ÙŠ ØªØ¨Ø­Ø« Ø¹Ù†Ù‡ ÙÙŠ ÙƒØªØ§Ø¨Ùƒ Ø§Ù„Ù‚Ø§Ø¯Ù…ØŸ"
        ]
        fallback_questions_en = [
            "What's the last book you read and enjoyed?",
            "Do you prefer fictional stories or realistic ones?",
            "What type of characters attract you most in novels?",
            "What mood are you looking for in your next book?"
        ]
        
        questions = fallback_questions_ar if user_lang == "ar" else fallback_questions_en
        return random.choice(questions)
def generate_no_books_smart_response(sid: str, session: Dict, preferences: Dict, language: str):
    """Ø¥Ù†Ø´Ø§Ø¡ Ø±Ø¯ Ø°ÙƒÙŠ Ø¹Ù†Ø¯Ù…Ø§ Ù„Ø§ ØªÙˆØ¬Ø¯ ÙƒØªØ¨"""
    
    prompt = f"""
    You're a helpful librarian. You couldn't find books matching the user's preferences.
    
    User wants: {preferences.get('wants', [])}
    User language: {language}
    
    Write a helpful message that:
    1. Apologizes briefly for not finding matching books
    2. Suggests alternative approaches (e.g., try different keywords, search in other language)
    3. Asks if they'd like to adjust their preferences
    4. Ends with an encouraging note
    
    Keep it friendly and helpful. Write in {language}.
    """
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7
        )
        
        reply = response.choices[0].message.content.strip()
        
    except Exception as e:
        print(f"âŒ Smart no-books response failed: {e}")
        if language == "ar":
            reply = "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£Ø¬Ø¯ ÙƒØªØ¨Ø§Ù‹ ØªØ·Ø§Ø¨Ù‚ ØªÙØ¶ÙŠÙ„Ø§ØªÙƒ. Ù‡Ù„ ØªØ±ÙŠØ¯ ØªØ¹Ø¯ÙŠÙ„ Ø´Ø±ÙˆØ· Ø§Ù„Ø¨Ø­Ø« Ø£Ùˆ Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©ØŸ"
        else:
            reply = "Sorry, I couldn't find books matching your preferences. Would you like to adjust your search or try English books?"
    
    update_conversation_history(session, "assistant", reply)
    touch_session(sid)
    
    return {
        "session_id": sid,
        "reply": reply,
        "books": [],
        "follow_up": True
    }
def should_save_preference(user_text: str, session: Dict) -> bool:
    """Ø§Ù„ØªØ­Ù‚Ù‚ Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠØ¬Ø¨ Ø­ÙØ¸ Ø§Ù„Ù†Øµ ÙƒØªÙØ¶ÙŠÙ„ (Ù…Ø¨Ø³Ø·)"""
    user_text = user_text.strip()
    
    # Ù„Ø§ ØªØ­ÙØ¸ Ù†ØµÙˆØµ Ù‚ØµÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹
    if len(user_text.split()) < 2:
        return False
    
    # Ù„Ø§ ØªØ­ÙØ¸ Ø¥Ø¬Ø§Ø¨Ø§Øª Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ù…Ø­Ø¯Ø¯Ø©
    last_assistant_msg = get_last_assistant_message(session["conversation_history"])
    if last_assistant_msg:
        last_msg_lower = last_assistant_msg.lower()
        # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø§Ù„Ù„ØºØ©
        if any(phrase in last_msg_lower for phrase in [
            "which language", "Ø£ÙŠ Ù„ØºØ©", "arabic or english"
        ]):
            return False
        # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø³Ø¤Ø§Ù„ Ù…ØªØ§Ø¨Ø¹Ø© Ø¨Ø³ÙŠØ·
        if "?" in last_assistant_msg and len(user_text.split()) < 4:
            return False
    
    return True

def save_user_preference_simple(session: Dict, user_text: str):
    """Ø­ÙØ¸ ØªÙØ¶ÙŠÙ„ Ù…Ø¨Ø³Ø·"""
    if not user_text or len(user_text.strip()) < 3:
        return
    
    # ØªØ­Ù‚Ù‚ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø© Ø¬Ø¯ÙŠØ¯Ø©
    existing_text = " ".join(session["user_prefs"].values()).lower()
    if user_text.lower() not in existing_text:
        pref_key = f"pref_{len(session['user_prefs']) + 1}"
        session["user_prefs"][pref_key] = user_text
        print(f"ğŸ’¾ Saved simple preference: '{user_text[:50]}...'")
def extract_keywords_fallback(conversation_history: List[Dict]) -> List[str]:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© ÙƒÙ€ fallback"""
    keywords = []
    for msg in conversation_history[-4:]:  # Ø¢Ø®Ø± 4 Ø±Ø³Ø§Ø¦Ù„
        if msg["role"] == "user":
            text = msg["content"].lower()
            # ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ø´Ø§Ø¦Ø¹Ø©
            common_keywords = [
                "book", "novel", "story", "history", "historical", 
                "fiction", "non-fiction", "arabic", "english",
                "pharaoh", "egypt", "ancient", "Ù‚Ø±Ø§Ø¡Ø©", "ÙƒØªØ§Ø¨",
                "Ø±ÙˆØ§ÙŠØ©", "Ù‚ØµØ©", "ØªØ§Ø±ÙŠØ®", "ÙØ±Ø¹ÙˆÙ†", "Ù…ØµØ±"
            ]
            
            for keyword in common_keywords:
                if keyword in text:
                    keywords.append(keyword)
    
    return list(set(keywords))[:5]  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
def filter_books_with_llm(books: List[Dict], preferences: Dict, language: str, top_k: int = 5) -> List[Dict]:
    """ÙÙ„ØªØ±Ø© Ø§Ù„ÙƒØªØ¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… LLM Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£Ù†Ø³Ø¨"""
    
    # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„ÙƒØªØ¨ Ù‚Ù„ÙŠÙ„Ø©ØŒ Ù„Ø§ Ù†Ø­ØªØ§Ø¬ ÙÙ„ØªØ±Ø©
    if len(books) <= top_k:
        return books[:top_k]
    
    # Ø¥Ø°Ø§ ÙƒØ§Ù† client ØºÙŠØ± Ù…ØªØ§Ø­ØŒ Ø§Ø³ØªØ®Ø¯Ù… ÙÙ„ØªØ±Ø© Ø¨Ø³ÙŠØ·Ø©
    if client is None:
        print("âš ï¸ OpenAI client not available, using simple filtering")
        # ÙÙ„ØªØ±Ø© Ø­Ø³Ø¨ Ø§Ù„Ù„ØºØ© Ø£ÙˆÙ„Ø§Ù‹
        lang_filtered = filter_books_by_language(books, language)
        return lang_filtered[:top_k] if lang_filtered else books[:top_k]
    
    # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ù€ LLM - Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… None
    books_info = []
    for i, book in enumerate(books[:30]):  # Ø­Ø¯ 30 ÙƒØªØ§Ø¨ Ù„Ù„ØªØ­Ù„ÙŠÙ„
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… None
        title = book.get("title", "Unknown Title") or "Unknown Title"
        authors = book.get("authors", "Unknown Author") or "Unknown Author"
        summary = book.get("short_summary", "") or ""
        
        books_info.append({
            "id": i,
            "title": title,
            "authors": authors,
            "summary": summary[:300] if summary else "",  # âœ… Ù…Ø¹Ø§Ù„Ø¬Ø© None
            "language": book.get("language", ""),
            "category": book.get("library_location", "").split("â€“")[0] if book.get("library_location") else ""
        })
    
    filter_prompt = f"""
    Select the most relevant books based on user preferences.
    
    USER PREFERENCES:
    - Wants: {preferences.get('wants', [])}
    - Does NOT want: {preferences.get('does_not_want', [])}
    - Language: {language}
    
    AVAILABLE BOOKS (ID, Title, Authors, Summary):
    {json.dumps(books_info, ensure_ascii=False)}
    
    TASK:
    1. Analyze each book against user preferences
    2. Select {top_k} books that best match what the user wants
    3. AVOID books that match what the user does NOT want
    4. Prioritize books in {language} if available
    
    Return a JSON array of book IDs (numbers only).
    Example: [3, 7, 1, 9, 4]
    """
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": filter_prompt}],
            response_format={"type": "json_object"},
            temperature=0.1,
            max_tokens=200
        )
        
        result = json.loads(response.choices[0].message.content)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ IDs Ù…Ù† Ø§Ù„Ø±Ø¯
        selected_ids = []
        if isinstance(result, dict):
            # Ø¬Ø±Ø¨ Ù…ÙØ§ØªÙŠØ­ Ù…Ø®ØªÙ„ÙØ©
            for key in ["selected_books", "books", "ids", "selected_ids", "recommendations"]:
                if key in result and isinstance(result[key], list):
                    selected_ids = result[key]
                    break
        elif isinstance(result, list):
            selected_ids = result
        
        print(f"ğŸ¤– LLM Selected Book IDs: {selected_ids}")
        
        # Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„ÙƒØªØ¨ Ø§Ù„Ù…Ø®ØªØ§Ø±Ø© Ù…Ø¹ Ø§Ù„ØªØ­Ù‚Ù‚
        selected_books = []
        for idx in selected_ids[:top_k]:
            if isinstance(idx, int) and 0 <= idx < len(books):
                selected_books.append(books[idx])
            elif isinstance(idx, str) and idx.isdigit():
                idx_int = int(idx)
                if 0 <= idx_int < len(books):
                    selected_books.append(books[idx_int])
        
        # Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… Ø§Ø®ØªÙŠØ§Ø± Ø£ÙŠ ÙƒØªØ¨ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø£ÙˆÙ„ top_k ÙƒØªØ¨
        if not selected_books:
            print("âš ï¸ LLM returned no valid selections, using top books")
            selected_books = books[:top_k]
        
        return selected_books
        
    except Exception as e:
        print(f"âŒ LLM filtering failed: {e}")
        import traceback
        traceback.print_exc()
        
        # Fallback: ÙÙ„ØªØ±Ø© Ø¨Ø³ÙŠØ·Ø© Ø­Ø³Ø¨ Ø§Ù„Ù„ØºØ©
        lang_filtered = filter_books_by_language(books, language)
        return lang_filtered[:top_k] if lang_filtered else books[:top_k]


def intelligent_book_search(user_query: str, preferences_analysis: Dict, language: str, top_k: int = 5):
    """Ø¨Ø­Ø« Ø°ÙƒÙŠ Ø¹Ù† Ø§Ù„ÙƒØªØ¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… LLM Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…"""
    
    # Ø¥Ø°Ø§ ÙƒØ§Ù† client ØºÙŠØ± Ù…ØªØ§Ø­
    if client is None:
        print("âš ï¸ OpenAI client not available, using regular search")
        results = find_top_k(user_query, k=top_k)
        # ÙÙ„ØªØ±Ø© Ø­Ø³Ø¨ Ø§Ù„Ù„ØºØ©
        return filter_books_by_language(results, language)[:top_k]
    
    # Step 1: ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… LLM
    try:
        query_optimization_prompt = f"""
        Create an optimized search query for finding books.
        
        User wants: {preferences_analysis.get('wants', [])}
        User query: "{user_query}"
        Language: {language}
        
        Create a concise search query in {language} that includes:
        - Main topic/keywords
        - Related terms
        - Genre/style
        
        Return ONLY the query string.
        """
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": query_optimization_prompt}],
            temperature=0.2,
            max_tokens=50
        )
        
        optimized_query = response.choices[0].message.content.strip()
        print(f"ğŸ” LLM Optimized Query: '{optimized_query}'")
        
    except Exception as e:
        print(f"âŒ Query optimization failed: {e}")
        optimized_query = user_query
    
    # Step 2: Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù…Ø­Ø³Ù†
    print(f"ğŸ” Searching with: '{optimized_query}'")
    try:
        initial_results = find_top_k(optimized_query, k=top_k * 3)  # Ø£Ø­Ø¶Ø± Ø£ÙƒØ«Ø±
    except Exception as e:
        print(f"âŒ Search failed: {e}")
        initial_results = []
    
    if not initial_results:
        print(f"âŒ No results with optimized query, trying original...")
        try:
            initial_results = find_top_k(user_query, k=top_k * 2)
        except Exception as e:
            print(f"âŒ Original search also failed: {e}")
            return []
    
    print(f"ğŸ“š Found {len(initial_results)} initial results")
    
    # Step 3: ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª - Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ÙƒØªØ¨ Ø§Ù„ØªÙŠ Ø¨Ù‡Ø§ Ù…Ø´Ø§ÙƒÙ„
    clean_books = []
    for book in initial_results:
        # ØªØ£ÙƒØ¯ Ø£Ù† Ø§Ù„ÙƒØªØ§Ø¨ Ù„Ù‡ Ø¹Ù†ÙˆØ§Ù† Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„
        if book.get("title"):
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… None
            if book.get("short_summary") is None:
                book["short_summary"] = ""
            clean_books.append(book)
    
    if not clean_books:
        print("âŒ No valid books found after cleaning")
        return []
    
    # Step 4: ÙÙ„ØªØ±Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… LLM (Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù‡Ù†Ø§Ùƒ Ù†ØªØ§Ø¦Ø¬ ÙƒØ§ÙÙŠØ©)
    if len(clean_books) > top_k:
        filtered_books = filter_books_with_llm(clean_books, preferences_analysis, language, top_k)
        print(f"âœ… LLM filtered to {len(filtered_books)} books")
        return filtered_books
    else:
        print(f"âœ… Using all {len(clean_books)} books (not enough for filtering)")
        return clean_books[:top_k]
    
def generate_intelligent_explanation(books: List[Dict], preferences: Dict, language: str) -> str:
    """ØªÙˆÙ„ÙŠØ¯ Ø´Ø±Ø­ Ø°ÙƒÙŠ Ù„Ù„ØªÙˆØµÙŠØ§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… LLM"""
    
    # Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙƒØªØ¨ Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… None
    books_info = []
    for book in books:
        title = book.get("title", "Unknown Book") or "Unknown Book"
        authors = book.get("authors", "Unknown Author") or "Unknown Author"
        summary = book.get("short_summary", "") or ""
        
        books_info.append({
            "title": title,
            "authors": authors,
            "summary": summary[:200] if summary else "",
            "language": book.get("language", "")
        })
    
    prompt = f"""
    You are a knowledgeable librarian helping a user find books.
    
    User Preferences Summary:
    {preferences.get('summary', 'Looking for interesting books')}
    
    User specifically wants: {preferences.get('wants', [])}
    User wants to avoid: {preferences.get('does_not_want', [])}
    
    Here are the books you're recommending:
    {json.dumps(books_info, ensure_ascii=False)}
    
    Write a personalized recommendation message that:
    1. Starts with a warm, engaging introduction
    2. Briefly explains why each book matches their preferences
    3. Highlights what makes each book special
    4. Ends with an open-ended question to continue the conversation
    
    Write in {language}, keep it conversational and friendly.
    """
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=500
        )
        
        return response.choices[0].message.content.strip()
        
    except Exception as e:
        print(f"âŒ Intelligent explanation failed: {e}")
        # Fallback ØªÙ‚Ù„ÙŠØ¯ÙŠ
        try:
            return generate_recommendation_explanation(
                preferences.get('summary', 'User preferences'),
                books,
                language
            )
        except:
            # Fallback Ø£Ø¨Ø³Ø·
            if language == "ar":
                return "Ù‡Ø°Ù‡ Ø¨Ø¹Ø¶ Ø§Ù„ÙƒØªØ¨ Ø§Ù„ØªÙŠ Ù‚Ø¯ ØªÙ†Ø§Ø³Ø¨ Ø§Ù‡ØªÙ…Ø§Ù…Ø§ØªÙƒ. Ø£ØªÙ…Ù†Ù‰ Ø£Ù† ØªØ¬Ø¯ Ù…Ø§ ØªØ¨Ø­Ø« Ø¹Ù†Ù‡!"
            else:
                return "Here are some books that might match your interests. I hope you find what you're looking for!"

def clean_book_data(book: Dict) -> Dict:
    """ØªÙ†Ø¸ÙŠÙ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒØªØ§Ø¨ - Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚ÙŠÙ… None"""
    cleaned = book.copy()
    
    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø­Ù‚ÙˆÙ„ Ø§Ù„Ù†ØµÙŠØ©
    text_fields = ["title", "authors", "short_summary", "publisher", "library_location"]
    for field in text_fields:
        if field in cleaned and cleaned[field] is None:
            cleaned[field] = ""
    
    if not cleaned.get("title"):
        cleaned["title"] = "Unknown Book"
    
    return cleaned

def smart_recommendation_system(sid: str, session: Dict, normalized_lang: str):
    """Ù†Ø¸Ø§Ù… ØªÙˆØµÙŠØ§Øª Ø°ÙƒÙŠ ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ LLM"""
    print("ğŸ§  [Stage] Using intelligent recommendation system...")
    
    try:
        # Step 1: ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙØ¶ÙŠÙ„Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… LLM
        preferences_analysis = analyze_preferences_with_llm(session["conversation_history"])
        
        # ØªØ­Ø¯ÙŠØ« Ù„ØºØ© Ø§Ù„Ø¬Ù„Ø³Ø© Ø¥Ø°Ø§ ØªÙ… Ø§ÙƒØªØ´Ø§ÙÙ‡Ø§
        detected_lang = preferences_analysis.get("language", normalized_lang)
        if detected_lang and "preferred_reading_lang" not in session:
            session["preferred_reading_lang"] = detected_lang
        
        # Step 2: Ø¨Ù†Ø§Ø¡ Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø°ÙƒÙŠ
        meaningful_prefs = [
            pref for pref in session["user_prefs"].values() 
            if pref and len(pref.split()) > 1
        ]
        
        user_query = " ".join(meaningful_prefs) if meaningful_prefs else "books"
        
        # Step 3: Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø°ÙƒÙŠ
        reading_lang = session.get("preferred_reading_lang", detected_lang)
        matched_books = intelligent_book_search(
            user_query, 
            preferences_analysis, 
            reading_lang
        )
        
        # Step 4: ØªÙ†Ø¸ÙŠÙ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒØªØ¨
        for book in matched_books:
            book = clean_book_data(book)
            ensure_cover(book)
        
        # Step 5: Ø¥Ø°Ø§ Ù„Ù… ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬ØŒ Ø­Ø§ÙˆÙ„ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ù…Ø®ØªÙ„ÙØ©
        if not matched_books:
            print("ğŸ”„ No books found, trying alternative strategies...")
            
            # Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© 1: Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙƒÙ„Ù…Ø§Øª Ø£Ø¹Ù…Ù‚
            broader_search_terms = generate_broader_search_terms(preferences_analysis)
            for term in broader_search_terms[:3]:
                matched_books = intelligent_book_search(term, preferences_analysis, reading_lang)
                if matched_books:
                    break
            
            # Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© 2: Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø£Ø®Ø±Ù‰
            if not matched_books and reading_lang == "ar":
                print("ğŸ”„ Trying English books as fallback...")
                matched_books = intelligent_book_search(
                    user_query, 
                    preferences_analysis, 
                    "en"
                )
        
        # Step 6: ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø´Ø±Ø­ Ø§Ù„Ø°ÙƒÙŠ
        if matched_books:
            explanation = generate_intelligent_explanation(
                matched_books, 
                preferences_analysis, 
                normalized_lang
            )
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
            update_conversation_history(session, "assistant", explanation)
            session["recommended"] = True
            touch_session(sid)
            
            response = {
                "session_id": sid,
                "reply": explanation,
                "books": matched_books,
                "follow_up": True,
            }
            
            log_response("Intelligent Recommendation", response)
            return response
        
        else:
            # Ø¥Ø°Ø§ Ù„Ù… ØªÙˆØ¬Ø¯ ÙƒØªØ¨ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø·Ù„Ø§Ù‚
            return generate_no_books_smart_response(sid, session, preferences_analysis, normalized_lang)
            
    except Exception as e:
        print(f"âŒ Error in smart recommendation system: {e}")
        import traceback
        traceback.print_exc()
        
        # Ø±Ø¯ Ø®Ø·Ø£
        if normalized_lang == "ar":
            reply = "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ù†Ø¸Ø§Ù… Ø§Ù„ØªÙˆØµÙŠØ§Øª. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰."
        else:
            reply = "Sorry, there was an error in the recommendation system. Please try again."
        
        update_conversation_history(session, "assistant", reply)
        touch_session(sid)
        
        return {
            "session_id": sid,
            "reply": reply,
            "books": [],
            "follow_up": True
        }
# ==================== EXTERNAL DEPENDENCIES (PLACEHOLDERS) ====================
import os
from dotenv import load_dotenv
from openai import OpenAI
import requests
from dotenv import load_dotenv
from fastapi import FastAPI
from pydantic import BaseModel
import numpy as np
import json
from pathlib import Path
from sklearn.metrics.pairwise import cosine_similarity
from langdetect import detect

load_dotenv() 

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
GOOGLE_BOOKS_API_KEY = os.getenv("GOOGLE_BOOKS_API_KEY", "")  # optional

client = OpenAI(api_key=OPENAI_API_KEY)

BOOKS_FILE = "books_dataset_enriched.jsonl"
EMB_FILE = "books_embeddings.npy"
META_FILE = "books_metadata.json"
EMBED_MODEL = "text-embedding-3-small"
TOP_K = 4

# ------------------- LOAD DATA -------------------
def load_books(path: str):
    return [json.loads(line) for line in open(path, "r", encoding="utf-8")]

def load_embeddings():
    embeddings = np.load(EMB_FILE)
    with open(META_FILE, "r", encoding="utf-8") as f:
        metas = json.load(f)
    return embeddings, metas


# ------------ LOAD MULTIPLE DATASETS ------------
def load_multiple_jsonl(paths: List[str]):
    all_books = []
    for p in paths:
        print(f"Loading books from: {p}")
        all_books.extend(load_books(p))
    return all_books

def load_multiple_embeddings(emb_paths: List[str], meta_paths: List[str]):
    all_embs = []
    all_metas = []

    for emb_file, meta_file in zip(emb_paths, meta_paths):
        print(f"Loading embeddings from: {emb_file}")
        embs = np.load(emb_file)
        all_embs.append(embs)

        print(f"Loading metadata from: {meta_file}")
        with open(meta_file, "r", encoding="utf-8") as f:
            metas = json.load(f)
        all_metas.extend(metas)

    # concatenate embeddings vertically
    final_embs = np.vstack(all_embs)
    return final_embs, all_metas


# ---------- LOAD DATASETS ----------
books_raw = load_multiple_jsonl([
    "books_dataset_enriched.jsonl",
    "second_dataset_clean.jsonl"
])

embeddings, metas = load_multiple_embeddings(
    ["books_embeddings.npy", "second_dataset_embeddings.npy"],
    ["books_metadata.json", "second_dataset_metadata.json"]
)

# ------------------- HELPERS -------------------
# def embed_text(text: str):
#     resp = client.embeddings.create(model=EMBED_MODEL, input=text)
#     return resp.data[0].embedding

def embed_text(text: str):
    """ØªÙˆÙ„ÙŠØ¯ embedding Ù„Ù„Ù†Øµ - Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡"""
    if not text or not text.strip():
        print("âš ï¸ Empty text for embedding")
        # Ø¥Ø±Ø¬Ø§Ø¹ embedding ÙØ§Ø±Øº Ø¨Ø§Ù„Ø­Ø¬Ù… Ø§Ù„ØµØ­ÙŠØ­
        if embeddings is not None and len(embeddings) > 0:
            return [0.0] * embeddings.shape[1]
        else:
            return [0.0] * 1536  # Ø§Ù„Ø­Ø¬Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ
    
    if client is None:
        print("âš ï¸ OpenAI client not available for embedding")
        if embeddings is not None and len(embeddings) > 0:
            return [0.0] * embeddings.shape[1]
        else:
            return [0.0] * 1536
    
    try:
        resp = client.embeddings.create(
            model=EMBED_MODEL,
            input=text
        )
        return resp.data[0].embedding
    except Exception as e:
        print(f"âŒ Error generating embedding: {e}")
        # Fallback embedding
        if embeddings is not None and len(embeddings) > 0:
            return [0.0] * embeddings.shape[1]
        else:
            return [0.0] * 1536
# def find_top_k(query: str, k: int = TOP_K):
#     query_emb = np.array(embed_text(query), dtype=np.float32).reshape(1, -1)
#     sims = cosine_similarity(query_emb, embeddings)[0]
#     idx = np.argsort(sims)[::-1][:k]
#     results = []
#     for i in idx:
#         m = metas[int(i)].copy()
#         m["_score"] = float(sims[int(i)])
#         results.append(m)
#     return results

def find_top_k(query: str, k: int = TOP_K):
    """Ø¨Ø­Ø« Ø¹Ù† Ø£ÙØ¶Ù„ K ÙƒØªØ¨ Ù…Ø´Ø§Ø¨Ù‡Ø© Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… - Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡"""
    
    if embeddings is None or len(embeddings) == 0:
        print("âŒ No embeddings loaded")
        return []
    
    if not metas or len(metas) == 0:
        print("âŒ No metadata loaded")
        return []
    
    try:
        # ØªÙˆÙ„ÙŠØ¯ embedding Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
        query_emb = np.array(embed_text(query), dtype=np.float32).reshape(1, -1)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
        sims = cosine_similarity(query_emb, embeddings)[0]
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ø¹Ù„Ù‰ K Ù…Ø¤Ø´Ø±Ø§Øª
        top_indices = np.argsort(sims)[::-1][:k]
        
        results = []
        for idx in top_indices:
            idx_int = int(idx)
            if 0 <= idx_int < len(metas):
                m = metas[idx_int].copy()
                m["_score"] = float(sims[idx_int])
                results.append(m)
            else:
                print(f"âš ï¸ Warning: Index {idx_int} out of bounds for metadata (size {len(metas)})")
        
        print(f"ğŸ” Search for '{query[:50]}...' returned {len(results)} results")
        return results
        
    except Exception as e:
        print(f"âŒ Error in find_top_k: {e}")
        import traceback
        traceback.print_exc()
        return []
    
def get_cover_google(isbn: str):
    if not isbn:
        return None
    try:
        url = f"https://www.googleapis.com/books/v1/volumes?q=isbn:{isbn}"
        r = requests.get(url, timeout=6)
        data = r.json()
        items = data.get("items")
        if items:
            vol = items[0].get("volumeInfo", {})
            imgs = vol.get("imageLinks", {})
            for k in ("extraLarge", "large", "medium", "small", "thumbnail"):
                if imgs.get(k):
                    return imgs.get(k)
        return None
    except:
        return None

def get_cover_openlibrary(isbn: str):
    if not isbn:
        return None
    return f"https://covers.openlibrary.org/b/isbn/{isbn}-L.jpg"

def ensure_cover(book: Dict[str, Any]):
    if book.get("cover_url"):
        return book["cover_url"]
    
    isbn = book.get("isbn","")
    cover = None
    
    if isbn and isbn != "N/A" and isbn != "null":
        cover = get_cover_google(isbn) or get_cover_openlibrary(isbn)
        if cover:
            print(f"âœ… Found cover by ISBN: {isbn}")
    
    if not cover:
        cover = get_cover_by_title(book.get('title', ''), book.get('authors', ''))
        if cover:
            print(f"âœ… Found cover by title: {book.get('title', '')}")
    
    book["cover_url"] = cover
    return cover
import requests
import urllib.parse

def get_cover_by_title(title: str, authors: str = ""):
    try:
        clean_title = title.strip().replace('"', '').replace(':', '')
        query = f"intitle:{clean_title}"
        
        if authors:
            first_author = authors.split('/')[0].split(',')[0].strip()
            clean_author = first_author.replace('"', '').replace(':', '')
            query += f" inauthor:{clean_author}"
        
        encoded_query = urllib.parse.quote(query)
        url = f"https://www.googleapis.com/books/v1/volumes?q={encoded_query}&maxResults=3"
        
        print(f"ğŸ” Searching for cover: {query}")
        
        r = requests.get(url, timeout=10)
        data = r.json()
        
        items = data.get("items", [])
        print(f"ğŸ“š Found {len(items)} items")
        
        if items:
            # Ù†Ø¬Ø±Ø¨ ÙƒÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¹Ø´Ø§Ù† Ù†Ù„Ø§Ù‚ÙŠ ÙˆØ§Ø­Ø¯Ø© ÙÙŠÙ‡Ø§ ØµÙˆØ±Ø©
            for item in items:
                vol = item.get("volumeInfo", {})
                imgs = vol.get("imageLinks", {})
                
                # Ù†Ø·Ø¨Ø¹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙƒØªØ§Ø¨ Ù„Ù„Øªdebug
                found_title = vol.get('title', '')
                found_authors = vol.get('authors', [])
                print(f"   ğŸ“– Found: '{found_title}' by {found_authors}")
                
                for k in ("extraLarge", "large", "medium", "small", "thumbnail"):
                    if imgs.get(k):
                        print(f"   âœ… Found cover: {imgs.get(k)}")
                        return imgs.get(k)
        
        print("   âŒ No covers found")
        return None
        
    except Exception as e:
        print(f"   âŒ Error in get_cover_by_title: {e}")
        return None
# ==================== RUN APP ====================
# import uvicorn

# if __name__ == "__main__":
#     uvicorn.run("app:app", host="127.0.0.1", port=8000, reload=False)